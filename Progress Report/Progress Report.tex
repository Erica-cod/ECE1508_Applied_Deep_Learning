\documentclass{article}
% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
\usepackage[final]{neurips}
% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Question Answering --- Task-Specific Models vs. LLMs}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Zongyan Yao\\
  Department of Electrical and Computer Engineering\\
  University of Toronto\\
  \texttt{zongyan.yao@mail.utoronto.ca}
  \And
  Zhengyang Li\\
  Department of Electrical and Computer Engineering\\
  University of Toronto\\
  \texttt{zhengyang.li@mail.utoronto.ca}
  \And
  Qiwen Lin\\
  Department of Electrical and Computer Engineering\\
  University of Toronto\\
  \texttt{qw.lin@mail.utoronto.ca}
}

\begin{document}


\maketitle

\begin{abstract}
Large Language Models (LLMs) have shown strong performance on open-domain question answering, but they often lack the domain-specific precision required for specialized tasks. This project investigates whether fine-tuning smaller and mid-sized pretrained transformer models can outperform general-purpose LLMs on a narrow recipe-focused benchmark. Using the Recipe-MPR dataset of 500 multi-perspective cooking questions, we fine-tune several encoder-based models (BERT, DistilBERT, RoBERTa \cite{devlin2019bert,sanh2019distilbert,liu2019roberta}) and decoder-based models (Llama \cite{touvron2023llama} and Qwen2.5--7B \cite{qwen2024qwen2}) with parameter-efficient methods such as LoRA \cite{hu2021lora}. Our goal is to exceed a 65\% accuracy threshold and to study trade-offs between accuracy, efficiency, and model capacity.

Initial results show that all fine-tuned encoder models exceed the target, with BERT-large achieving 91.4\% accuracy and DistilBERT providing strong performance at lower computational cost. The LoRA-tuned Qwen2.5--7B model achieves 100\% accuracy, and fine-tuned Llama variants reach up to 84\%, while GPT-3 embeddings \cite{text_embeddings2022} perform significantly worse. These findings highlight the benefits of domain adaptation and provide a clear basis for deeper analysis in the final report.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) such as BERT and GPT have demonstrated impressive performance on open-domain natural language tasks \cite{devlin2019bert,vaswani2017attention}, but their general-purpose nature can limit reliability in narrow domains. In the recipe domain, small differences in wording can affect ingredient substitutions, cooking steps, or safety-related advice, and general LLMs may not always capture these nuances consistently.

To address this, we study whether fine-tuning smaller or mid-sized pretrained transformer models on a recipe-specific dataset can outperform a general-purpose LLM baseline. We use the Recipe-MPR dataset, which consists of 500 user queries with five candidate answers per query, covering multiple reasoning types (analogical, specific, commonsense, temporal, and negated). Our aim in this progress stage is to clearly specify the problem, describe our model and training design, report the main results obtained so far, and outline the remaining milestones rather than to deliver a polished final narrative.

\section{Preliminaries and Problem Formulation}

\subsection{Problem Definition}

We formulate the task as a multiple-choice question answering problem over recipes. For each example, we are given:
\begin{itemize}
    \item a query $q$ (a user question about recipes or cooking), and
    \item a set of five candidate answers $A = \{a_1, a_2, a_3, a_4, a_5\}$.
\end{itemize}
The goal is to learn a function
\[
f_\theta(q, A) \rightarrow y, \quad y \in \{1,2,3,4,5\},
\]
that predicts the index of the correct answer. We model this as a five-class classification problem using transformer-based architectures \cite{vaswani2017attention}.

\subsection{Design Components and Objectives}

To make the project structure clear, we explicitly summarize our main design components:

\begin{itemize}
    \item \textbf{Dataset:} Recipe-MPR (500 examples), split into 80/10/10 for train/validation/test.
    \item \textbf{Models:}
    \begin{itemize}
        \item Encoder models: BERT-base, BERT-large \cite{devlin2019bert}, DistilBERT \cite{sanh2019distilbert}, RoBERTa-base \cite{liu2019roberta}.
        \item Decoder models: Llama-3.2-1B, Llama-3.2-3B \cite{touvron2023llama} and Qwen2.5--7B \cite{qwen2024qwen2}.
    \end{itemize}
    \item \textbf{Training Strategy:}
    \begin{itemize}
        \item Full fine-tuning for encoder models.
        \item LoRA-based parameter-efficient fine-tuning for large decoder models \cite{hu2021lora}.
        \item Comparison with zero-shot LLMs and a GPT-3 embedding baseline \cite{text_embeddings2022}.
    \end{itemize}
    \item \textbf{Metrics:} Accuracy (primary) and macro F1-score (planned for final report).
    \item \textbf{Targets:} 
    \begin{itemize}
        \item Exceed a 65\% accuracy baseline.
        \item Analyze trade-offs between accuracy, model size, training time, and inference cost.
    \end{itemize}
\end{itemize}

\subsection{Relevant Background Concepts}

\paragraph{Transformer Architecture.}
All models in this work are based on the transformer architecture \cite{vaswani2017attention}, which uses multi-head self-attention, feed-forward layers, positional encodings, and layer normalization to model contextual dependencies.

\paragraph{Fine-Tuning and LoRA.}
Encoder models (e.g., BERT and RoBERTa) are fine-tuned end-to-end \cite{devlin2019bert,liu2019roberta}, whereas large decoder LLMs (Llama, Qwen) are adapted using LoRA \cite{hu2021lora}, which adds low-rank trainable adapters to reduce memory and compute while preserving most of the pretrained weights.

\paragraph{Embedding Baseline.}
We also experiment with GPT-3 embeddings \cite{text_embeddings2022}, where queries and answers are mapped to vector representations, and the answer is chosen based on similarity in embedding space. This provides a nonâ€“fine-tuned baseline for comparison against explicit model adaptation.

\section{Solution via Deep Learning}

\subsection{Dataset Preparation}

We preprocess the Recipe-MPR dataset by:
\begin{itemize}
    \item normalizing text (lowercasing and basic cleanup),
    \item tokenizing using each model's tokenizer,
    \item truncating or padding sequences to a fixed maximum length,
    \item splitting into 80\% training, 10\% validation, 10\% test.
\end{itemize}
This part of the pipeline is fully implemented and reused across all models to ensure fair comparison.

\subsection{Model and Training Pipeline}

We have implemented a unified training pipeline in PyTorch with HuggingFace Transformers for both encoder and decoder models.

\paragraph{Encoder models.}
BERT-base, BERT-large, DistilBERT, and RoBERTa-base \cite{devlin2019bert,sanh2019distilbert,liu2019roberta} are fine-tuned by adding a classification head on top of the [CLS] representation and training with cross-entropy loss.

\paragraph{Decoder models.}
Llama-3.2-1B, Llama-3.2-3B \cite{touvron2023llama} and Qwen2.5--7B \cite{qwen2024qwen2} are adapted using LoRA \cite{hu2021lora} with 4-bit quantization to fit within 8--32GB GPUs. We frame the task as sequence-to-sequence or causal LM scoring over the candidate answers.

\subsection{Current Progress and Preliminary Results}

At this stage of the project, the following components are completed:

\begin{itemize}
    \item All preprocessing, tokenization, and dataset splitting.
    \item Full fine-tuning of BERT-family models (BERT-base, BERT-large, DistilBERT, RoBERTa-base).
    \item LoRA-based fine-tuning of Qwen2.5--7B and Llama-3.2 models.
    \item Implementation and evaluation of a GPT-3 embedding baseline \cite{text_embeddings2022}.
    \item Test-set evaluations for all models and a cross-model comparison.
\end{itemize}

Preliminary results (on our current test split) show:

\begin{itemize}
    \item \textbf{Qwen2.5--7B (LoRA)}: 100\% accuracy (no errors on the evaluated test set).
    \item \textbf{BERT-large}: 91.4\% accuracy.
    \item \textbf{DistilBERT}: 82.4\% accuracy.
    \item \textbf{Llama-3.2-3B (LoRA)}: 84.0\% accuracy; Llama-1B (LoRA) reaches 78.0\%.
    \item GPT-3 embeddings: substantially lower accuracy ($\sim$55\%), confirming that fine-tuning brings clear gains over simple embedding-based similarity.
\end{itemize}

All fine-tuned models exceed the 65\% threshold, and several exceed 80\%, indicating that our design choices are effective for this dataset.

\subsection{Remaining Milestones}

Although the core training and evaluation pipeline is complete, several important steps remain for the final report:

\begin{itemize}
    \item Perform detailed error analysis across query types (analogical, specific, commonsense, temporal, negated).
    \item Compare qualitative outputs of models (e.g., typical failure cases for smaller vs.\ larger models).
    \item Analyze trade-offs in more depth, including:
    \begin{itemize}
        \item training time vs.\ accuracy,
        \item VRAM requirements vs.\ performance,
        \item inference latency vs.\ model size.
    \end{itemize}
    \item Produce visualizations (e.g., bar plots, confusion matrices, accuracy vs.\ parameters).
    \item Integrate all findings into a polished final report with a more formal discussion and conclusion.
\end{itemize}

From a project management perspective, we have completed the major technical components (data pipeline, training, and initial evaluation) and are now focused on analysis, comparison, and final documentation.

{
\small
\bibliographystyle{plain}
\bibliography{references}
}


\end{document}