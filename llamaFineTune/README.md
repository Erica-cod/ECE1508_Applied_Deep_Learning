# Recipe-MPR Llama3 å¾®è°ƒå®éªŒ

æœ¬é¡¹ç›®å¯¹æ¯”äº†å¾®è°ƒçš„ Llama3-MPR-SFT æ¨¡å‹ä¸ GPT-3 Embedding Baseline åœ¨ Recipe-MPR æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚

## ğŸ“Š å®éªŒç»“æœ

| æ¨¡å‹ | å‡†ç¡®ç‡ |
|------|--------|
| **Llama3-MPR-SFT** | **84.00%** |
| GPT-3 Embedding | 54.55% |

**æå‡**: +29.45 ä¸ªç™¾åˆ†ç‚¹

## ğŸ“ é¡¹ç›®ç»“æ„

```
llamaFineTune/
â”œâ”€â”€ data/                          # æ•°æ®é›†
â”‚   â”œâ”€â”€ train.jsonl               # è®­ç»ƒé›† (300 samples)
â”‚   â”œâ”€â”€ valid.jsonl               # éªŒè¯é›† (100 samples)
â”‚   â””â”€â”€ test.jsonl                # æµ‹è¯•é›† (100 samples)
â”‚
â”œâ”€â”€ Recipe-MPR/                    # åŸå§‹æ•°æ®é›†å’Œå‚è€ƒä»£ç 
â”‚   â””â”€â”€ data/500QA.json           # åŸå§‹ 500 ä¸ªé£Ÿè°±é—®ç­”
â”‚
â”œâ”€â”€ outputs/                       # è®­ç»ƒè¾“å‡º
â”‚   â””â”€â”€ llama3-mpr-sft/
â”‚       â””â”€â”€ final/                # æœ€ç»ˆå¾®è°ƒæ¨¡å‹
â”‚
â”œâ”€â”€ compare-result/                # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ æœ€ç»ˆå®éªŒæŠ¥å‘Š.md            # å®Œæ•´å®éªŒæŠ¥å‘Š â­
â”‚   â”œâ”€â”€ detailed_errors.json      # è¯¦ç»†é”™è¯¯æ¡ˆä¾‹
â”‚   â”œâ”€â”€ overall_stats.csv         # æ€»ä½“ç»Ÿè®¡
â”‚   â””â”€â”€ stats_by_type.csv         # æŒ‰æŸ¥è¯¢ç±»å‹ç»Ÿè®¡
â”‚
â”œâ”€â”€ prep_mpr.py                    # æ•°æ®å‡†å¤‡è„šæœ¬
â”œâ”€â”€ train_sft.py                   # æ¨¡å‹å¾®è°ƒè„šæœ¬
â”œâ”€â”€ eval_mpr.py                    # è¯„ä¼°å¾®è°ƒæ¨¡å‹
â”œâ”€â”€ eval_embedding_baseline.py     # è¯„ä¼° embedding baseline
â”œâ”€â”€ compare_runs.py                # å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹ç»“æœ
â”‚
â”œâ”€â”€ embeddings_with_aspects.json   # GPT-3 é¢„è®¡ç®—çš„ embeddings
â”œâ”€â”€ mpr_preds.jsonl               # Llama3-MPR-SFT é¢„æµ‹ç»“æœ
â””â”€â”€ emb_preds.jsonl               # GPT-3 Embedding é¢„æµ‹ç»“æœ
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å‡†å¤‡æ•°æ®

```bash
python prep_mpr.py \
    --infile Recipe-MPR/data/500QA.json \
    --outdir data \
    --seed 42
```

### 2. è®­ç»ƒæ¨¡å‹

```bash
python train_sft.py
```

éœ€è¦ï¼š
- Llama-3.2-3B-Instruct åŸºç¡€æ¨¡å‹ï¼ˆæ”¾åœ¨ `~/models/Llama-3.2-3B-Instruct/`ï¼‰
- 8GB+ GPU æ˜¾å­˜
- çº¦ 12-15 åˆ†é’Ÿè®­ç»ƒæ—¶é—´

### 3. è¯„ä¼°æ¨¡å‹

**è¯„ä¼°å¾®è°ƒæ¨¡å‹**ï¼š
```bash
python eval_mpr.py \
    --data data/test.jsonl \
    --model_dir ~/models/Llama-3.2-3B-Instruct \
    --adapter_dir outputs/llama3-mpr-sft/final \
    --save_pred mpr_preds.jsonl
```

**è¯„ä¼° Embedding Baseline**ï¼š
```bash
python eval_embedding_baseline.py \
    --data data/test.jsonl \
    --raw_json Recipe-MPR/data/500QA.json \
    --emb embeddings_with_aspects.json \
    --save_pred emb_preds.jsonl
```

### 4. å¯¹æ¯”ç»“æœ

```bash
python compare_runs.py \
    --raw500 Recipe-MPR/data/500QA.json \
    --mpr_preds mpr_preds.jsonl \
    --emb_preds emb_preds.jsonl
```

## ğŸ“– æŸ¥çœ‹ç»“æœ

å®Œæ•´çš„å®éªŒæŠ¥å‘Šåœ¨ `compare-result/æœ€ç»ˆå®éªŒæŠ¥å‘Š.md`

## ğŸ”‘ å…³é”®å‘ç°

1. **æ•°æ®åå·®é—®é¢˜**ï¼šå‘ç°å¹¶ä¿®å¤äº†åŸå§‹æ•°æ®ä¸­æ‰€æœ‰ç­”æ¡ˆéƒ½åœ¨ä½ç½® A çš„é—®é¢˜
2. **å¾®è°ƒæ•ˆæœæ˜¾è‘—**ï¼š3B æ¨¡å‹ç»è¿‡å¾®è°ƒåè¶…è¶Šäº†é€šç”¨çš„ GPT-3 Embedding
3. **ä»»åŠ¡ç‰¹åŒ–é‡è¦**ï¼šé’ˆå¯¹ä»»åŠ¡çš„å¾®è°ƒæ¯”æ¨¡å‹è§„æ¨¡æ›´å…³é”®

## ğŸ“š ä¾èµ–ç¯å¢ƒ

```
transformers
datasets
peft
bitsandbytes
torch
numpy
```

## ğŸ“„ è®¸å¯

æœ¬é¡¹ç›®åŸºäº Recipe-MPR æ•°æ®é›†è¿›è¡Œå®éªŒã€‚

