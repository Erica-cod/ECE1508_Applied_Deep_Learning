# -*- coding: utf-8 -*-
import os, json, re, argparse, random, torch
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

try:
    from peft import PeftModel

    _has_peft = True
except Exception:
    _has_peft = False

A2E = "ABCDE"


def load_jsonl(path: str) -> List[Dict]:
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(l) for l in f]


def first_A2E(s: str) -> Optional[str]:
    m = re.search(r"[A-E]", s.upper())
    return m.group(0) if m else None


def ensure_answer_suffix(txt: str) -> str:
    t = txt.rstrip()
    return t if t.endswith("Answer:") else (t + "\nAnswer:")


def format_fewshot(records: List[Dict], k: int) -> str:
    """records: [{'text': '...Answer:', 'label': 'C'}, ...]"""
    if k <= 0: return ""
    blocks = []
    for r in records[:k]:
        blocks.append(ensure_answer_suffix(r["text"]) + " " + r["label"].strip())
    return "\n\n".join(blocks) + "\n\n"


def build_prompt(example: Dict, fewshot_block: str) -> str:
    # Our sample structure: {"text": "...Answer:", "label": "C"} (from prep_mpr.py)
    return fewshot_block + ensure_answer_suffix(example["text"])


def load_model_and_tokenizer(model_dir: str, adapter_dir: Optional[str], offload_folder: Optional[str],
                             use_flash_attn: bool):
    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype="bfloat16" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else "float16",
    )
    kwargs = dict(
        quantization_config=bnb,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    if offload_folder:
        kwargs["offload_folder"] = offload_folder
        kwargs["max_memory"] = {0: "7.2GiB", "cpu": "32GiB"}
    if use_flash_attn:
        kwargs["attn_implementation"] = "flash_attention_2"
    else:
        kwargs["attn_implementation"] = "sdpa"

    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=False, local_files_only=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    model = AutoModelForCausalLM.from_pretrained(model_dir, local_files_only=True, **kwargs)
    model.eval()

    if adapter_dir:
        if not _has_peft:
            raise RuntimeError("PEFT is required to load LoRA adapter. Please install: pip install peft")
        model = PeftModel.from_pretrained(model, adapter_dir, is_trainable=False)
        model.eval()

    return tok, model


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data", default="data/test.jsonl", help="Test set path (generated by prep_mpr.py)")
    ap.add_argument("--model_dir", required=True, help="Local base model directory, e.g. ~/models/Llama-3.2-3B-Instruct")
    ap.add_argument("--adapter_dir", default=None, help="LoRA directory, e.g. outputs/llama3-mpr-sft/final; leave empty for zero-shot")
    ap.add_argument("--fewshot", type=int, default=0, help="Number of few-shot examples (sampled from train.jsonl)")
    ap.add_argument("--fewshot_source", default="data/train.jsonl", help="Few-shot sample source file")
    ap.add_argument("--limit", type=int, default=0, help="Evaluate only first N samples (0=all)")
    ap.add_argument("--max_new_tokens", type=int, default=2)
    ap.add_argument("--offload_folder", default=None, help="Specify when GPU memory is tight, e.g. offload")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--show_errors", type=int, default=5, help="Print first K errors (0=don't print)")
    ap.add_argument("--use_flash_attn", action="store_true", help="Enable if flash-attn2 is installed")
    ap.add_argument("--save_pred", default=None, help="Save predictions for each question to jsonl")
    args = ap.parse_args()

    random.seed(args.seed)

    # Few-shot block
    fewshot_block = ""
    if args.fewshot > 0:
        fs = load_jsonl(args.fewshot_source)
        random.shuffle(fs)
        fewshot_block = format_fewshot(fs, args.fewshot)

    # Load model/tokenizer
    model_dir = os.path.expanduser(args.model_dir)
    adapter_dir = os.path.expanduser(args.adapter_dir) if args.adapter_dir else None
    tok, model = load_model_and_tokenizer(model_dir, adapter_dir, args.offload_folder, args.use_flash_attn)

    # Load test set
    data = load_jsonl(args.data)
    if args.limit and args.limit > 0:
        data = data[:args.limit]

    correct = 0
    wrong_cases = []
    total = len(data)

    # Open output file before loop
    out_f = open(args.save_pred, "w", encoding="utf-8") if args.save_pred else None

    for i, ex in enumerate(data, 1):
        prompt = build_prompt(ex, fewshot_block)
        inputs = tok(prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=False,
                temperature=None,
                top_p=None
            )
        gen = tok.decode(out[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
        pred = first_A2E(gen or "")
        gold = ex["label"].strip().upper()

        import re
        m = re.search(r"Question:\n(.*?)(?:\n\nOptions:|\Z)", ex["text"], flags=re.S)
        query_text = m.group(1).strip() if m else None

        if out_f:
            out_f.write(json.dumps({
                "query": query_text,
                "pred_letter": pred,  # A/B/C/D/E or None
                "gold_letter": gold  # A/B/C/D/E
            }, ensure_ascii=False) + "\n")

        if pred == gold:
            correct += 1
        else:
            if len(wrong_cases) < args.show_errors:
                # Extract question text, remove few-shot part
                wrong_cases.append({
                    "pred": pred,
                    "gold": gold,
                    "gen": gen.strip(),
                    "text_preview": ex["text"].split("\nOptions:\n")[0][-200:] + " ...",
                })

        if i % 50 == 0:
            print(f"[{i}/{total}] running acc: {correct / i:.4f}")

    # After loop
    if out_f: out_f.close()

    acc = correct / total if total else 0.0
    print("=" * 60)
    print(f"Adapter: {adapter_dir or 'None (zero-shot)'}")
    print(f"Data: {args.data}  |  Total: {total}")
    print(f"Accuracy: {acc:.4f}")

    if wrong_cases:
        print("\n-- Wrong cases (first {}):".format(len(wrong_cases)))
        for j, w in enumerate(wrong_cases, 1):
            print(f"[{j}] pred={w['pred']} gold={w['gold']}  gen='{w['gen']}'")
            print(f"    {w['text_preview']}")
    print("=" * 60)


if __name__ == "__main__":
    main()
