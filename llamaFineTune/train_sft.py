import os, json, inspect, argparse
from datasets import Dataset
from transformers import (AutoTokenizer, AutoModelForCausalLM,
                          BitsAndBytesConfig, TrainingArguments,
                          DataCollatorForLanguageModeling, Trainer)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

def load_jsonl(p):
    rows=[]
    with open(p,'r',encoding='utf-8') as f:
        for line in f:
            rows.append(json.loads(line))
    return rows

def ensure_answer_suffix(txt:str)->str:
    t = txt.rstrip()
    if not t.endswith("Answer:"):
        t = t + "\nAnswer:"
    return t

def join_prompt_and_label(rec):
    # Our sample format: {"text": "...Answer:", "label":"C"}
    return {"text": ensure_answer_suffix(rec["text"]) + " " + rec["label"].strip()}

def make_training_args(**kwargs):
    # Compatible with old/new transformers parameter names: eval_strategy vs evaluation_strategy
    sig = inspect.signature(TrainingArguments.__init__)
    if "eval_strategy" in sig.parameters:
        kwargs["eval_strategy"] = kwargs.pop("evaluation_strategy", "epoch")
    else:
        kwargs["evaluation_strategy"] = kwargs.pop("eval_strategy", "epoch")
    return TrainingArguments(**kwargs)

def main():
    # Parse arguments
    ap = argparse.ArgumentParser(description='Fine-tune Llama model for recipe recommendation')
    ap.add_argument('--model_dir', default='~/models/Llama-3.2-3B-Instruct',
                    help='Base model path')
    ap.add_argument('--output_dir', default='outputs/llama3-mpr-sft',
                    help='Output directory')
    ap.add_argument('--epochs', type=int, default=5,
                    help='Number of training epochs')
    args = ap.parse_args()
    
    MODEL_DIR = os.path.expanduser(args.model_dir)
    OUTPUT_DIR = args.output_dir
    
    print(f"Model path: {MODEL_DIR}")
    print(f"Output path: {OUTPUT_DIR}")
    print(f"Training epochs: {args.epochs}")
    print()
    
    # 1) Load data (generated by prep_mpr.py)
    train_rows = [join_prompt_and_label(r) for r in load_jsonl("data/train.jsonl")]
    valid_rows = [join_prompt_and_label(r) for r in load_jsonl("data/valid.jsonl")]
    ds_train = Dataset.from_list(train_rows)
    ds_valid = Dataset.from_list(valid_rows)

    # 2) Tokenizer
    tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False, local_files_only=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # 3) Quantization config (8GB GPU friendly)
    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype="bfloat16"
    )

    # 4) Load base model
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_DIR,
        quantization_config=bnb,
        device_map="auto",
        offload_folder="offload",
        max_memory={0: "8GiB", "cpu": "32GiB"},
        attn_implementation="sdpa",
        low_cpu_mem_usage=True,
        local_files_only=True
    )
    model.config.use_cache = False  # Disable KV cache during training to save memory

    # 5) LoRA adaptation
    peft_cfg = LoraConfig(
        r=4, lora_alpha=16, lora_dropout=0.05, bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
    )
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    model = get_peft_model(model, peft_cfg)
    model.print_trainable_parameters()

    # 6) Tokenize data (Causal LM: labels == input_ids)
    def tok_fn(batch):
        out = tok(batch["text"], truncation=True, max_length=128)
        out["labels"] = out["input_ids"].copy()
        return out

    ds_train = ds_train.map(tok_fn, remove_columns=["text"], batched=True)
    ds_valid = ds_valid.map(tok_fn, remove_columns=["text"], batched=True)

    # 7) Data collator (no MLM)
    collator = DataCollatorForLanguageModeling(tok, mlm=False)

    # 8) Training arguments
    training_args = make_training_args(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        num_train_epochs=args.epochs,
        logging_steps=10,
        eval_strategy="no",
        save_strategy="epoch",
        bf16=False, fp16=True,
        gradient_checkpointing=True,
        optim="paged_adamw_32bit",
        report_to=[]  # Disable wandb etc.
    )

    # 9) Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds_train,
        eval_dataset=ds_valid,
        data_collator=collator,
        tokenizer=tok
    )

    trainer.train()
    trainer.save_model(OUTPUT_DIR + "/final")  # Save LoRA adapter
    print(f"\nâœ… Model saved to: {OUTPUT_DIR}/final")

if __name__ == "__main__":
    main()
